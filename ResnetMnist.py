# -*- coding: utf-8 -*-
"""convnet-vgg16.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/github/rasbt/deeplearning-models/blob/master/pytorch_ipynb/cnn/cnn-resnet34-mnist.ipynb

Deep Learning Models -- A collection of various deep learning architectures, models, and tips for TensorFlow and PyTorch in Jupyter Notebooks.
- Author: Sebastian Raschka
- GitHub Repository: https://github.com/rasbt/deeplearning-models
"""

# Commented out IPython magic to ensure Python compatibility.
# %load_ext watermark
# %watermark -a 'Sebastian Raschka' -v -p torch

"""# Model Zoo -- ResNet-34 MNIST Digits Classifier

### Network Architecture

The network in this notebook is an implementation of the ResNet-34 [1] architecture on the MNIST digits dataset (http://yann.lecun.com/exdb/mnist/) to train a handwritten digit classifier.  


References
    
- [1] He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 770-778). ([CVPR Link](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html))

- [2] http://yann.lecun.com/exdb/mnist/

![](https://github.com/rasbt/deeplearning-models/blob/master/pytorch_ipynb/images/resnets/resnet34/resnet34-arch.png?raw=1)

The following figure illustrates residual blocks with skip connections such that the input passed via the shortcut matches the dimensions of the main path's output, which allows the network to learn identity functions.

![](https://github.com/rasbt/deeplearning-models/blob/master/pytorch_ipynb/images/resnets/resnet-ex-1-1.png?raw=1)


The ResNet-34 architecture actually uses residual blocks with skip connections such that the input passed via the shortcut matches is resized to dimensions of the main path's output. Such a residual block is illustrated below:

![](https://github.com/rasbt/deeplearning-models/blob/master/pytorch_ipynb/images/resnets/resnet-ex-1-2.png?raw=1)

For a more detailed explanation see the other notebook, [resnet-ex-1.ipynb](resnet-ex-1.ipynb).

## Imports
"""

import os
import time

import numpy as np
import pandas as pd

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader

from torchvision import datasets
from torchvision import transforms

import matplotlib.pyplot as plt
from PIL import Image


if torch.cuda.is_available():
    torch.backends.cudnn.deterministic = True

"""## Model Settings"""

##########################
### SETTINGS
##########################

# Hyperparameters
RANDOM_SEED = 1
LEARNING_RATE = 0.001
BATCH_SIZE = 128
NUM_EPOCHS = 10

# Architecture
NUM_FEATURES = 28*28
NUM_CLASSES = 10

# Other
DEVICE = "cuda:0"
GRAYSCALE = True

"""### MNIST Dataset"""

##########################
### MNIST DATASET
##########################

# Note transforms.ToTensor() scales input images
# to 0-1 range
train_dataset = datasets.MNIST(root='data',
                               train=True,
                               transform=transforms.ToTensor(),
                               download=True)

test_dataset = datasets.MNIST(root='data',
                              train=False,
                              transform=transforms.ToTensor())


train_loader = DataLoader(dataset=train_dataset,
                          batch_size=BATCH_SIZE,
                          shuffle=True)

test_loader = DataLoader(dataset=test_dataset,
                         batch_size=BATCH_SIZE,
                         shuffle=False)

# Checking the dataset
for images, labels in train_loader:
    print('Image batch dimensions:', images.shape)
    print('Image label dimensions:', labels.shape)
    break

device = torch.device(DEVICE)
torch.manual_seed(0)

for epoch in range(2):

    for batch_idx, (x, y) in enumerate(train_loader):

        print('Epoch:', epoch+1, end='')
        print(' | Batch index:', batch_idx, end='')
        print(' | Batch size:', y.size()[0])

        x = x.to(device)
        y = y.to(device)
        break

"""The following code cell that implements the ResNet-34 architecture is a derivative of the code provided at https://pytorch.org/docs/0.4.0/_modules/torchvision/models/resnet.html."""

##########################
### MODEL
##########################


def conv3x3(in_planes, out_planes, stride=1):
    """3x3 convolution with padding"""
    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,
                     padding=1, bias=False)


class BasicBlock(nn.Module):
    expansion = 1

    def __init__(self, inplanes, planes, stride=1, downsample=None):
        super(BasicBlock, self).__init__()
        self.conv1 = conv3x3(inplanes, planes, stride)
        self.bn1 = nn.BatchNorm2d(planes)
        self.relu = nn.ReLU(inplace=True)
        self.conv2 = conv3x3(planes, planes)
        self.bn2 = nn.BatchNorm2d(planes)
        self.downsample = downsample
        self.stride = stride

    def forward(self, x):
        residual = x

        out = self.conv1(x)
        out = self.bn1(out)
        out = self.relu(out)

        out = self.conv2(out)
        out = self.bn2(out)

        if self.downsample is not None:
            residual = self.downsample(x)

        out += residual
        out = self.relu(out)

        return out




class ResNet(nn.Module):

    def __init__(self, block, layers, num_classes, grayscale):
        self.inplanes = 64
        if grayscale:
            in_dim = 1
        else:
            in_dim = 3
        super(ResNet, self).__init__()
        self.conv1 = nn.Conv2d(in_dim, 64, kernel_size=7, stride=2, padding=3,
                               bias=False)
        self.bn1 = nn.BatchNorm2d(64)
        self.relu = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        self.layer1 = self._make_layer(block, 64, layers[0])
        self.layer2 = self._make_layer(block, 128, layers[1], stride=2)
        self.layer3 = self._make_layer(block, 256, layers[2], stride=2)
        self.layer4 = self._make_layer(block, 512, layers[3], stride=2)
        self.avgpool = nn.AvgPool2d(7, stride=1)
        self.fc = nn.Linear(512 * block.expansion, num_classes)

        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels
                m.weight.data.normal_(0, (2. / n)**.5)
            elif isinstance(m, nn.BatchNorm2d):
                m.weight.data.fill_(1)
                m.bias.data.zero_()

    def _make_layer(self, block, planes, blocks, stride=1):
        downsample = None
        if stride != 1 or self.inplanes != planes * block.expansion:
            downsample = nn.Sequential(
                nn.Conv2d(self.inplanes, planes * block.expansion,
                          kernel_size=1, stride=stride, bias=False),
                nn.BatchNorm2d(planes * block.expansion),
            )

        layers = []
        layers.append(block(self.inplanes, planes, stride, downsample))
        self.inplanes = planes * block.expansion
        for i in range(1, blocks):
            layers.append(block(self.inplanes, planes))

        return nn.Sequential(*layers)

    def forward(self, x):
        x = self.conv1(x)
        x = self.bn1(x)
        x = self.relu(x)
        x = self.maxpool(x)

        x = self.layer1(x)
        x = self.layer2(x)
        x = self.layer3(x)
        x = self.layer4(x)
        # because MNIST is already 1x1 here:
        # disable avg pooling
        #x = self.avgpool(x)

        x = x.view(x.size(0), -1)
        logits = self.fc(x)
        probas = F.softmax(logits, dim=1)
        return logits, probas



def resnet34(num_classes):
    """Constructs a ResNet-34 model."""
    model = ResNet(block=BasicBlock,
                   layers=[3, 4, 6, 3],
                   num_classes=NUM_CLASSES,
                   grayscale=GRAYSCALE)
    return model

torch.manual_seed(RANDOM_SEED)
model = resnet34(NUM_CLASSES)
model.to(DEVICE)

optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)

"""## Training"""

# Commented out IPython magic to ensure Python compatibility.
import torch
import time
import os

def save_checkpoint(state, filename="normaltraining.pth"):
    """Save checkpoint."""
    torch.save(state, filename)

def load_checkpoint(checkpoint, model, optimizer):
    """Load checkpoint."""
    model.load_state_dict(checkpoint['state_dict'])
    optimizer.load_state_dict(checkpoint['optimizer'])
    return checkpoint['epoch']

def compute_accuracy(model, data_loader, device):
    correct_pred, num_examples = 0, 0
    for i, (features, targets) in enumerate(data_loader):
        features = features.to(device)
        targets = targets.to(device)
        logits, probas = model(features)
        _, predicted_labels = torch.max(probas, 1)
        num_examples += targets.size(0)
        correct_pred += (predicted_labels == targets).sum()
    return correct_pred.float() / num_examples * 100

# Load checkpoint if exists and start from there
start_epoch = 0
if os.path.isfile('normaltraining.pth'):
    checkpoint = torch.load('normaltraining.pth')
    start_epoch = load_checkpoint(checkpoint, model, optimizer)
    print(f"Loaded checkpoint and starting from epoch {start_epoch+1}")

start_time = time.time()
for epoch in range(start_epoch, NUM_EPOCHS):
    model.train()
    for batch_idx, (features, targets) in enumerate(train_loader):
        features = features.to(DEVICE)
        targets = targets.to(DEVICE)

        # FORWARD AND BACK PROP
        logits, probas = model(features)
        cost = F.cross_entropy(logits, targets)
        optimizer.zero_grad()
        cost.backward()

        # UPDATE MODEL PARAMETERS
        optimizer.step()

        # LOGGING
        if not batch_idx % 50:
            print(f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | Batch {batch_idx:04d}/{len(train_loader):04d} | Cost: {cost:.4f}')

    model.eval()
    with torch.no_grad():  # Save memory during inference
        print(f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | Train: {compute_accuracy(model, train_loader, DEVICE):.3f}%')

    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')

    # Save checkpoint
    save_checkpoint({
        'epoch': epoch + 1,
        'state_dict': model.state_dict(),
        'optimizer': optimizer.state_dict(),
    })

print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')

# Save the final model
torch.save(model.state_dict(), 'final_model.pth')



"""## Evaluation"""

with torch.set_grad_enabled(False): # save memory during inference
    print('Test accuracy: %.2f%%' % (compute_accuracy(model, test_loader, device=DEVICE)))

for batch_idx, (features, targets) in enumerate(test_loader):

    features = features
    targets = targets
    break


nhwc_img = np.transpose(features[0], axes=(1, 2, 0))
nhw_img = np.squeeze(nhwc_img.numpy(), axis=2)
plt.imshow(nhw_img, cmap='Greys');

model.eval()
logits, probas = model(features.to(device)[0, None])
print('Probability 7 %.2f%%' % (probas[0][7]*100))

#Importing Adversarial attacks
from art.estimators.classification import PyTorchClassifier
from art.attacks.evasion import FastGradientMethod, ProjectedGradientDescent

class ModelWrapper(nn.Module):
    def __init__(self, model):
        super(ModelWrapper, self).__init__()
        self.model = model

    def forward(self, x):
        logits, _ = self.model(x)  # Assuming the model returns logits and probabilities
        return logits

model.eval()  # Set the model to evaluation mode


wrapped_model = ModelWrapper(model)

# Wrap the PyTorch model with ART's PyTorchClassifier
classifier = PyTorchClassifier(
    model=wrapped_model,
    clip_values=(0, 1),
    loss=torch.nn.CrossEntropyLoss(),
    optimizer=optimizer,
    input_shape=(1, 28, 28),
    nb_classes=NUM_CLASSES,
    device_type=DEVICE
)

"""#PROJECT GRADIENT DESCENT"""
import time
total_correct = 0
total_examples = 0

# Create PGD attack
pgd_attack = ProjectedGradientDescent(estimator=classifier, eps=0.3, max_iter=40)

# Ensure the model is in evaluation mode
model.eval()

#Starting time
start_time = time.time()

for images, labels in test_loader:
    # Convert images to NumPy array for ART
    images_np = images.numpy()

    # Generate adversarial examples with PGD
    x_test_adv_pgd = pgd_attack.generate(x=images_np)

    # Convert adversarial examples to torch tensors and move to the correct device
    x_test_adv_pgd_torch = torch.from_numpy(x_test_adv_pgd).to(DEVICE)
    labels = labels.to(DEVICE)

    # Perform inference on adversarial examples
    logits, _ = model(x_test_adv_pgd_torch)
    _, predictions = torch.max(logits, dim=1)

    # Update the accumulators
    total_correct += (predictions == labels).sum().item()
    total_examples += labels.size(0)

#End Timer
end_time = time.time()
# Calculate the overall accuracy
accuracy_pgd = total_correct / total_examples
print(f"Accuracy on PGD adversarial examples over the entire test set: {accuracy_pgd * 100:.2f}%")

#Total time
total_time = end_time- start_time
print(f"Total time required: {total_time:.2f} seconds")


"""#Carlini """
from art.attacks.evasion import  CarliniL2Method

total_correct = 0
total_examples = 0

# Create C&W L2 attack
cw_attack = CarliniL2Method(classifier=classifier, max_iter=100, binary_search_steps=5, learning_rate=0.01, batch_size=128)

# Ensure the model is in evaluation mode
model.eval()
start_time = time.time()

for images, labels in test_loader:
    # Convert images to NumPy array for ART
    images_np = images.numpy()

    # Generate adversarial examples for the current batch
    x_test_adv_cw = cw_attack.generate(x=images_np)

    # Convert adversarial examples back to PyTorch tensors and move to the correct device
    x_test_adv_cw_torch = torch.from_numpy(x_test_adv_cw).to(DEVICE)
    labels = labels.to(DEVICE)

    # Perform inference on adversarial examples
    logits, _ = model(x_test_adv_cw_torch)
    _, predictions = torch.max(logits, dim=1)

    # Update the accumulators
    total_correct += (predictions == labels).sum().item()
    total_examples += labels.size(0)

end_time = time.time()


# Calculate the overall accuracy
accuracy_cw = total_correct / total_examples
print(f"Accuracy on C&W adversarial examples over the entire test set: {accuracy_cw * 100:.2f}%")

# Calculate and print the total time
total_time = end_time - start_time
print(f"Total time required for C&W: {total_time:.2f} seconds")

# Commented out IPython magic to ensure Python compatibility.
# %watermark -iv
